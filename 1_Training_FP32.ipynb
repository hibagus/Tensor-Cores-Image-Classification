{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE382V - Hardware Architecture for Machine Learning\n",
    "## NVIDIA Tensor Cores for Accelerating Machine Learning Workload\n",
    "\n",
    "## Notebook 1 - Model Training with FP32\n",
    "\n",
    "In this notebook, we will try to preprocess our dataset so that we can use them to train our neural network model. We will use pretrained ResNet-50 model. ResNet is a convolutional neural network that becomes a backbone for many computer vision tasks. The pretrained ResNet-50 model has been trained on more than a million images from ImageNet database [6]. The network has 50 layers and can classify images into 1000 object categories. If you want to learn more about ResNet-50, please visit [7]. The training step in this Notebook is a modified version from [8].\n",
    "\n",
    "A pre-trained model contains the weights and biases that represent the features of dataset it was trained on. These learned features are often transferable to different data. Our task is to retrain the model to fit our needs: classifying cats and dogs image. By using a pre-trained model, we can save time and compute resources as someone else already spent countless time and compute resources to train the model to learn a lot of features. The training in this notebook will use FP32. It means that we will not use Tensor Cores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "We need to import some libraries which are needed to perform some functions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variable\n",
    "Here, we define global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir       = './'\n",
    "raw_dir        = f'{data_dir}/raw'\n",
    "raw_dogs_dir   = f'{raw_dir}/dogs'\n",
    "raw_cats_dir   = f'{raw_dir}/cats'\n",
    "train_dir      = f'{data_dir}/train'\n",
    "train_dogs_dir = f'{train_dir}/dogs'\n",
    "train_cats_dir = f'{train_dir}/cats'\n",
    "val_dir        = f'{data_dir}/val'\n",
    "val_dogs_dir   = f'{val_dir}/dogs'\n",
    "val_cats_dir   = f'{val_dir}/cats'\n",
    "log_dir        = f'{data_dir}/log'\n",
    "chk_dir        = f'{data_dir}/checkpoint'\n",
    "test_dir       = f'{data_dir}/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Initialization\n",
    "We will use GPU to train our model. The TACC Maverick2 V100 Compute Node is equipped with two NVIDIA Tesla V100 GPUs. In this assignment, we will only use one of them. If there is no GPU available, we will revert back to use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Augmentation and Normalization\n",
    "Before we train our model, we need to preprocess our dataset. We will do Image Data Augmentation which is a method to artificially enchance the size and quality of dataset. The augmentantion can create variations of the data that can improve the ability of the model to fit on the real world scenario. You can learn more about Image Data Augmentation in [9]. The variations of data can consist of rotation, flip, noise, brightness, and contrast. We also need to normalize our dataset. We will use the default value that is used in PyTorch example [10]. We define different transformation for training dataset and validation dataset. We do not do augmentation on validation dataset. We also convert the data into PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Handler\n",
    "In this section, we will define the method how we will handle the dataset and feed them into both training and validation process. We will use PyTorch dataloader to feed the dataset into the training and validation of the model. \n",
    "\n",
    "The batch size is the number of data (image) taken from dataset to train and update our model in each iteration. For example, if we have 128 images in our dataset and we have batch size of 16, then there will be 8 iterations to train our model in one epoch. In each iteration, we take 16 images, train our model, and update model parameter. You can adjust the batch size as a part of hyperparameter tuning. In this assignment, we will only adjust the batch size according to the available GPU memory. The NVIDIA Tesla V100 has 16GB of HBM2 memory and it should be able to store 1024 images in single batch (if we use FP32). You can change the batch size according to your GPU Memory Size. \n",
    "\n",
    "The workers are basically the number of background process that the data loader can use. Since TACC Maverick2 V100 Compute Node has 96 hardware threads (48 cores with HyperThreading), we put 96 as the number of workers. Feel free to change the number of workers according to the available hardware thread on your workstation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Change as needed ######################\n",
    "batch_size  = 1024\n",
    "num_workers = 96\n",
    "##############################################################\n",
    "\n",
    "# Define the data transformation\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Define the data loader\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=num_workers)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "# Print the statistics\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(f'Train image size: {dataset_sizes[\"train\"]}')\n",
    "print(f'Validation image size: {dataset_sizes[\"val\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pre-Trained Model\n",
    "We download the pre-trained model of ResNet-50. By using pre-Trained model, we can save time and computational resource to train our model. Then, this pre-trained model will be trained using our dataset. We also define a checkpoint location which allows us to save our model after we have trained it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained model of ResNet-50\n",
    "model_conv  = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Define the checkpoint location to save the trained model\n",
    "check_point = f'{chk_dir}/model-checkpoint-fp32.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Components\n",
    "Before we go to train our model, we need to define several components. We need to define the Criterion, Optimizer, and Learning Rate Scheduler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We change the parameter of the final fully connected layer.\n",
    "# We have to keep the number of input features to this layer.\n",
    "# We change the output features from this layer into 2 features (i.e., we only have two classes).\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Copy the model into GPU memory\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "# Choose the Criterion as Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimize only the parameters of the final fully connected layer since we have changed them.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# This is our learning rate scheduler. Decay learning rate by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Function\n",
    "We define our training function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, timestamp):\n",
    "    since = time.time()\n",
    "    writer = SummaryWriter('log/'+timestamp+'-fp32')\n",
    "    \n",
    "    # Initialization\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = math.inf\n",
    "    best_acc = 0.\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                \n",
    "                if phase == 'train' :\n",
    "                    writer.add_scalar('Train/Current_Running_Loss', loss.item(), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Current_Running_Corrects', torch.sum(preds == labels.data), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Accum_Running_Loss', running_loss, epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Accum_Running_Corrects', running_corrects, epoch*len(dataloaders[phase])+i)\n",
    "                else :\n",
    "                    writer.add_scalar('Validation/Current_Running_Loss', loss.item(), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Current_Running_Corrects', torch.sum(preds == labels.data), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Running_Loss', epoch_loss, epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Running_Corrects', epoch_acc, epoch*len(dataloaders[phase])+i)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'train' :\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "            else :\n",
    "                writer.add_scalar('Validation/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Validation/Accuracy', epoch_acc, epoch)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        writer.flush()\n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f} Best val loss: {:.4f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model\n",
    "Finally, we can train our model. You can adjust the number of epochs to train our model. An epoch is one training and one validation. We will use large learning rate at the first epoch. The learning rate will slowly be decreased as we run more epoch to find the global optimum. You are free to adjust the number of epoch as it will only take around 1 minute to run 1 epoch. You can monitor the training progress in TensorBoard. TensorBoard data will be updated at the end of each epoch.\n",
    "\n",
    "You will also need to monitor the GPU Memory Usage. Open your first terminal in MobaXterm and execute command nvidia-smi. This command will give you the list of all GPUs installed in the compute node and their status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Change as needed ######################\n",
    "num_epochs = 14\n",
    "##############################################################\n",
    "\n",
    "today = datetime.datetime.today() \n",
    "timestamp = today.strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Start the training\n",
    "model_conv, best_val_loss, best_val_acc = train_model(model_conv,\n",
    "                                                      criterion,\n",
    "                                                      optimizer_conv,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      num_epochs,\n",
    "                                                      timestamp)\n",
    "\n",
    "# Save the trained model for future use.\n",
    "torch.save({'model_state_dict': model_conv.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_conv.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\n",
    "            }, check_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End\n",
    "This is the end of Notebook 1. Please move forward to Notebook 2 where we will make prediction from our trained model.\n",
    "\n",
    "!!IMPORTANT!! To close this Notebook, you have to use File -> Close and Halt. With this way, the Python process associated with this Notebook will also be killed.\n",
    "\n",
    "Version 0.7  - January 7th, 2020 - ©2020 hanindhito@bagus.my.id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
