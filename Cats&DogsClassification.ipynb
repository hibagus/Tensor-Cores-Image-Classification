{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cats and Dogs Classification\n",
    "\n",
    "This notebook will guide you to train a simple neural network to perform classification of image whether it is a dog or a cat. We will use Pytorch as a framework to build and train our neural network. The training will make use of two Nvidia Tesla V100 GPUs available on TACC server to accelerate the training of our neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Install\n",
    "This section will install all of the libraries needed for this assignment. Please make sure that all of libraries are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y pytorch torchvision cuda100 -c pytorch\n",
    "!pip install googledrivedownloader requests\n",
    "!pip install split-folders tqdm\n",
    "!pip install matplotlib\n",
    "!pip install tensorboardX tensorboard\n",
    "!pip install cxxfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Import\n",
    "Load all of the libraries needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import split_folders\n",
    "import tqdm\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Download\n",
    "We will use dataset from Kaggle. It contains 12,500 images of cats and 12,500 images of dogs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Unzipping the Training Dataset\n",
    "gdd.download_file_from_google_drive(file_id='1TgS3BLPIoc3FHUBrvp6rXaz6g1UJz_2E',\n",
    "                                    dest_path='./raw.zip',\n",
    "                                    showsize=False,\n",
    "                                    overwrite=True,\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Unzipping the Testing Dataset\n",
    "gdd.download_file_from_google_drive(file_id='1JRMQY-gXp43ag65nP7HMNFEKhkJTxykw',\n",
    "                                    dest_path='./test.zip',\n",
    "                                    showsize=False,\n",
    "                                    overwrite=True,\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "We will preprocess the dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder\n",
    "os.makedirs(\"./raw/cats\"   ,exist_ok=True)\n",
    "os.makedirs(\"./raw/dogs\"   ,exist_ok=True)\n",
    "os.makedirs(\"./train\"      ,exist_ok=True)\n",
    "os.makedirs(\"./train/cats\" ,exist_ok=True)\n",
    "os.makedirs(\"./train/dogs\" ,exist_ok=True)\n",
    "os.makedirs(\"./val\"        ,exist_ok=True)\n",
    "os.makedirs(\"./val/cats\"   ,exist_ok=True)\n",
    "os.makedirs(\"./val/dogs\"   ,exist_ok=True)\n",
    "os.makedirs(\"./log\"        ,exist_ok=True)\n",
    "os.makedirs(\"./checkpoint\" ,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the folder path in the variable\n",
    "data_dir = './'\n",
    "raw_dir   = f'{data_dir}/raw'\n",
    "raw_dogs_dir = f'{raw_dir}/dogs'\n",
    "raw_cats_dir = f'{raw_dir}/cats'\n",
    "train_dir = f'{data_dir}/train'\n",
    "train_dogs_dir = f'{train_dir}/dogs'\n",
    "train_cats_dir = f'{train_dir}/cats'\n",
    "val_dir = f'{data_dir}/val'\n",
    "val_dogs_dir = f'{val_dir}/dogs'\n",
    "val_cats_dir = f'{val_dir}/cats'\n",
    "log_dir = f'{data_dir}/log'\n",
    "chk_dir = f'{data_dir}/checkpoint'\n",
    "test_dir = f'{data_dir}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the cats into cats folder and dogs into dogs folder\n",
    "files = os.listdir(raw_dir)\n",
    "for f in files:\n",
    "    catImageSearch = re.search(\"cat\", f)\n",
    "    dogImageSearch = re.search(\"dog\", f)\n",
    "    if catImageSearch:\n",
    "        shutil.move(f'{raw_dir}/{f}', raw_cats_dir)\n",
    "    elif dogImageSearch:\n",
    "        shutil.move(f'{raw_dir}/{f}', raw_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset for training and validation\n",
    "percentage_for_training = 0.8\n",
    "percentage_for_validation = 0.2\n",
    "random_seed = 12345\n",
    "split_folders.ratio(f'{raw_dir}', output=\"./\", seed=random_seed, ratio=(percentage_for_training, percentage_for_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Augmentation and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'dogs']\n",
      "Train image size: 20000\n",
      "Validation image size: 5000\n"
     ]
    }
   ],
   "source": [
    "# Configure the batch size and workers\n",
    "batch_size = 1024\n",
    "num_workers= 96\n",
    "\n",
    "# Checkpoint file. We currently don't use the checkpoint file\n",
    "check_point = f'{chk_dir}/checkpoint.tar'\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=num_workers)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names) # => ['cats', 'dogs']\n",
    "print(f'Train image size: {dataset_sizes[\"train\"]}')\n",
    "print(f'Validation image size: {dataset_sizes[\"val\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the device that will be used. In this assignment, we will only use single GPU.\n",
    "# If there is no GPU available, we will use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (Single Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, timestamp):\n",
    "    since = time.time()\n",
    "    writer = SummaryWriter('log/'+timestamp+'-single')\n",
    "    \n",
    "    # Initialization\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = math.inf\n",
    "    best_acc = 0.\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                \n",
    "                if phase == 'train' :\n",
    "                    writer.add_scalar('Train/Current_Running_Loss', loss.item(), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Current_Running_Corrects', torch.sum(preds == labels.data), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Accum_Running_Loss', running_loss, epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Accum_Running_Corrects', running_corrects, epoch*len(dataloaders[phase])+i)\n",
    "                else :\n",
    "                    writer.add_scalar('Validation/Current_Running_Loss', loss.item(), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Current_Running_Corrects', torch.sum(preds == labels.data), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Running_Loss', epoch_loss, epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Running_Corrects', epoch_acc, epoch*len(dataloaders[phase])+i)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'train' :\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "            else :\n",
    "                writer.add_scalar('Validation/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Validation/Accuracy', epoch_acc, epoch)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f} Best val loss: {:.4f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Pretrained Resnet50 Model\n",
    "model_conv = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Copy the pretrained model to GPU (if available) for further training\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "# Choose the Criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "today = datetime.datetime.today() \n",
    "timestamp = today.strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Start the training\n",
    "model_conv, best_val_loss, best_val_acc = train_model(model_conv,\n",
    "                                                      criterion,\n",
    "                                                      optimizer_conv,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      num_epochs,\n",
    "                                                      timestamp)\n",
    "\n",
    "# Save the trained model for future use.\n",
    "torch.save({'model_state_dict': model_conv.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_conv.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\n",
    "            }, check_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference (Single Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all test data directory\n",
    "test_data_files = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_test_transforms(inp):\n",
    "    out = transforms.functional.resize(inp, [224,224])\n",
    "    out = transforms.functional.to_tensor(out)\n",
    "    out = transforms.functional.normalize(out, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    return out\n",
    "\n",
    "def predict_dog_prob_of_single_instance(model, tensor):\n",
    "    batch = torch.stack([tensor])\n",
    "    # Send the input to GPU (if any)\n",
    "    batch = batch.to(device)\n",
    "    softMax = nn.Softmax(dim = 1)\n",
    "    preds = softMax(model(batch))\n",
    "    return preds[0,1].item()\n",
    "\n",
    "def test_data_from_fname(fname):\n",
    "    im = Image.open(f'{test_dir}/{fname}')\n",
    "    return apply_test_transforms(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_test_images = 4\n",
    "if(num_of_test_images<2) :\n",
    "    num_of_test_images = 2\n",
    "image_inferenced   = 0\n",
    "fig, ax = plt.subplots(num_of_test_images, figsize=(num_of_test_images*5, num_of_test_images*5))\n",
    "fig.tight_layout(pad=5)\n",
    "\n",
    "for fname in test_data_files :    \n",
    "    im         = Image.open(f'{test_dir}/{fname}')\n",
    "    imstar     = apply_test_transforms(im)    \n",
    "    outputs = predict_dog_prob_of_single_instance(model_conv, imstar)\n",
    "    ax[image_inferenced].imshow(im)\n",
    "    ax[image_inferenced].axis('on')\n",
    "    if(outputs<0.5) :\n",
    "        ax[image_inferenced].set_title('predicted: cat \\n probability: ' + str(1-outputs))\n",
    "    else :\n",
    "        ax[image_inferenced].set_title('predicted: dog \\n probability: ' + str(outputs))\n",
    "    image_inferenced += 1\n",
    "    if(image_inferenced>=num_of_test_images) :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (Mixed Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, timestamp):\n",
    "    since = time.time()\n",
    "    writer = SummaryWriter('log/'+timestamp+'-mixed')\n",
    "    \n",
    "    # Initialization\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = math.inf\n",
    "    best_acc = 0.\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(torch.float16).to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                print(type(running_loss))\n",
    "                print(type(running_corrects))\n",
    "                \n",
    "                if phase == 'train' :\n",
    "                    writer.add_scalar('Train/Current_Running_Loss', loss.item(), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Current_Running_Corrects', torch.sum(preds == labels.data), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Accum_Running_Loss', running_loss, epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Accum_Running_Corrects', running_corrects, epoch*len(dataloaders[phase])+i)\n",
    "                else :\n",
    "                    writer.add_scalar('Validation/Current_Running_Loss', loss.item(), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Current_Running_Corrects', torch.sum(preds == labels.data), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Running_Loss', epoch_loss, epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Running_Corrects', epoch_acc, epoch*len(dataloaders[phase])+i)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'train' :\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "            else :\n",
    "                writer.add_scalar('Validation/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Validation/Accuracy', epoch_acc, epoch)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f} Best val loss: {:.4f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Pretrained Resnet50 Model\n",
    "model_conv = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Copy the pretrained model to GPU (if available) for further training\n",
    "model_conv = model_conv.to(torch.float16).to(device)\n",
    "\n",
    "# Choose the Criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/06156/bagus/anaconda3/envs/CatsandDogsClassification/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "train Loss: 0.4895 Acc: 0.7947\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "<class 'torch.Tensor'>\n",
      "val Loss: 0.2240 Acc: 0.9678\n",
      "New best model found!\n",
      "New record loss: 0.224006640625, previous record loss: inf\n",
      "\n",
      "Training complete in 0m 45s\n",
      "Best val Acc: 0.9678 Best val loss: 0.2240\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "num_epochs = 1\n",
    "today = datetime.datetime.today() \n",
    "timestamp = today.strftime('%Y%m%d-%H%M%S')\n",
    "model_conv, best_val_loss, best_val_acc = train_model(model_conv,\n",
    "                                                      criterion,\n",
    "                                                      optimizer_conv,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      num_epochs,\n",
    "                                                      timestamp)\n",
    "\n",
    "# Save the trained model for future use.\n",
    "torch.save({'model_state_dict': model_conv.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_conv.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\n",
    "            }, check_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference (Mixed Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
