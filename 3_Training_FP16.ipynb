{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE382V - Hardware Architecture for Machine Learning\n",
    "## NVIDIA Tensor Cores for Accelerating Machine Learning Workload\n",
    "\n",
    "## Notebook 3 - Model Training with FP16\n",
    "\n",
    "In this notebook, we will try to modify our training process into FP16 so that it can use Tensor Cores. We need to train our model in order to take advantage of Tensor Cores. We will still use pre-trained ResNet-50 and the same dataset in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "We need to import some libraries which are needed to perform some functions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variable\n",
    "Here, we define global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir       = './'\n",
    "raw_dir        = f'{data_dir}/raw'\n",
    "raw_dogs_dir   = f'{raw_dir}/dogs'\n",
    "raw_cats_dir   = f'{raw_dir}/cats'\n",
    "train_dir      = f'{data_dir}/train'\n",
    "train_dogs_dir = f'{train_dir}/dogs'\n",
    "train_cats_dir = f'{train_dir}/cats'\n",
    "val_dir        = f'{data_dir}/val'\n",
    "val_dogs_dir   = f'{val_dir}/dogs'\n",
    "val_cats_dir   = f'{val_dir}/cats'\n",
    "log_dir        = f'{data_dir}/log'\n",
    "chk_dir        = f'{data_dir}/checkpoint'\n",
    "test_dir       = f'{data_dir}/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Initialization\n",
    "We will use GPU to train our model. The TACC Maverick2 V100 Compute Node is equipped with two NVIDIA Tesla V100 GPUs. In this assignment, we will only use one of them. If there is no GPU available, we will revert back to use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Augmentation and Normalization\n",
    "Before we train our model, we need to preprocess our dataset. We will do Image Data Augmentation which is a method to artificially enchance the size and quality of dataset. The augmentantion can create variations of the data that can improve the ability of the model to fit on the real world scenario. You can learn more about Image Data Augmentation in [9]. The variations of data can consist of rotation, flip, noise, brightness, and contrast. We also need to normalize our dataset. We will use the default value that is used in PyTorch example [10]. We define different transformation for training dataset and validation dataset. We do not do augmentation on validation dataset. We also convert the data into PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Handler\n",
    "In this section, we will define the method how we will handle the dataset and feed them into both training and validation process. We will use PyTorch dataloader to feed the dataset into the training and validation of the model. \n",
    "\n",
    "The batch size is the number of data (image) taken from dataset to train and update our model in each iteration. For example, if we have 128 images in our dataset and we have batch size of 16, then there will be 8 iterations to train our model in one epoch. In each iteration, we take 16 images, train our model, and update model parameter. You can adjust the batch size as a part of hyperparameter tuning. In this assignment, we will only adjust the batch size according to the available GPU memory. The NVIDIA Tesla V100 has 16GB of HBM2 memory and it should be able to store 1024 images in single batch (if we use FP32). You can change the batch size according to your GPU Memory Size. \n",
    "\n",
    "The workers are basically the number of background process that the data loader can use. Since TACC Maverick2 V100 Compute Node has 96 hardware threads (48 cores with HyperThreading), we put 96 as the number of workers. Feel free to change the number of workers according to the available hardware thread on your workstation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'dogs']\n",
      "Train image size: 20000\n",
      "Validation image size: 5000\n"
     ]
    }
   ],
   "source": [
    "###################### Change as needed ######################\n",
    "batch_size  = 1024\n",
    "num_workers = 96\n",
    "##############################################################\n",
    "\n",
    "# Define the data transformation\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Define the data loader\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=num_workers)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "# Print the statistics\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(f'Train image size: {dataset_sizes[\"train\"]}')\n",
    "print(f'Validation image size: {dataset_sizes[\"val\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pre-Trained Model\n",
    "We download the pre-trained model of ResNet-50. By using pre-Trained model, we can save time and computational resource to train our model. Then, this pre-trained model will be trained using our dataset. We also define a checkpoint location which allows us to save our model after we have trained it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained model of ResNet-50\n",
    "model_conv  = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Define the checkpoint location to save the trained model\n",
    "check_point = f'{chk_dir}/model-checkpoint-fp16.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Components\n",
    "Before we go to train our model, we need to define several components. We need to define the Criterion, Optimizer, and Learning Rate Scheduler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We change the parameter of the final fully connected layer.\n",
    "# We have to keep the number of input features to this layer.\n",
    "# We change the output features from this layer into 2 features (i.e., we only have two classes).\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Copy the model into GPU memory\n",
    "model_conv = model_conv.to(torch.float16).to(device)\n",
    "\n",
    "# Choose the Criterion as Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimize only the parameters of the final fully connected layer since we have changed them.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# This is our learning rate scheduler. Decay learning rate by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Function\n",
    "We define our training function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, timestamp):\n",
    "    since = time.time()\n",
    "    writer = SummaryWriter('log/'+timestamp+'-fp16')\n",
    "    \n",
    "    # Initialization\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = math.inf\n",
    "    best_acc = 0.\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(torch.float16).to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                \n",
    "                if phase == 'train' :\n",
    "                    writer.add_scalar('Train/Current_Running_Loss', loss.item(), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Current_Running_Corrects', torch.sum(preds == labels.data), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Accum_Running_Loss', running_loss, epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Train/Accum_Running_Corrects', running_corrects, epoch*len(dataloaders[phase])+i)\n",
    "                else :\n",
    "                    writer.add_scalar('Validation/Current_Running_Loss', loss.item(), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Current_Running_Corrects', torch.sum(preds == labels.data), epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Running_Loss', epoch_loss, epoch*len(dataloaders[phase])+i)\n",
    "                    writer.add_scalar('Validation/Running_Corrects', epoch_acc, epoch*len(dataloaders[phase])+i)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'train' :\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "            else :\n",
    "                writer.add_scalar('Validation/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Validation/Accuracy', epoch_acc, epoch)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        writer.flush()\n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f} Best val loss: {:.4f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model\n",
    "Finally, we can train our model. You can adjust the number of epochs to train our model. An epoch is one training and one validation. We will use large learning rate at the first epoch. The learning rate will slowly be decreased as we run more epoch to find the global optimum. You are free to adjust the number of epoch as it will only take around 1 minute to run 1 epoch. You can monitor the training progress in TensorBoard. TensorBoard data will be updated at the end of each epoch.\n",
    "\n",
    "You will also need to monitor the GPU Memory Usage. Open your first terminal in MobaXterm and execute command nvidia-smi. This command will give you the list of all GPUs installed in the compute node and their status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/13\n",
      "----------\n",
      "train Loss: 0.0832 Acc: 0.9783\n",
      "val Loss: 0.0840 Acc: 0.9776\n",
      "New best model found!\n",
      "New record loss: 0.0839548828125, previous record loss: inf\n",
      "\n",
      "Epoch 1/13\n",
      "----------\n",
      "train Loss: 0.0834 Acc: 0.9779\n",
      "val Loss: 0.0840 Acc: 0.9772\n",
      "\n",
      "Epoch 2/13\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.9789\n",
      "val Loss: 0.0839 Acc: 0.9776\n",
      "New best model found!\n",
      "New record loss: 0.0839197265625, previous record loss: 0.0839548828125\n",
      "\n",
      "Epoch 3/13\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.9793\n",
      "val Loss: 0.0840 Acc: 0.9772\n",
      "\n",
      "Epoch 4/13\n",
      "----------\n",
      "train Loss: 0.0834 Acc: 0.9782\n",
      "val Loss: 0.0837 Acc: 0.9772\n",
      "New best model found!\n",
      "New record loss: 0.083683203125, previous record loss: 0.0839197265625\n",
      "\n",
      "Epoch 5/13\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.9780\n",
      "val Loss: 0.0842 Acc: 0.9774\n",
      "\n",
      "Epoch 6/13\n",
      "----------\n",
      "train Loss: 0.0828 Acc: 0.9795\n",
      "val Loss: 0.0841 Acc: 0.9774\n",
      "\n",
      "Epoch 7/13\n",
      "----------\n",
      "train Loss: 0.0834 Acc: 0.9783\n",
      "val Loss: 0.0837 Acc: 0.9776\n",
      "\n",
      "Epoch 8/13\n",
      "----------\n",
      "train Loss: 0.0830 Acc: 0.9783\n",
      "val Loss: 0.0839 Acc: 0.9774\n",
      "\n",
      "Epoch 9/13\n",
      "----------\n",
      "train Loss: 0.0833 Acc: 0.9784\n",
      "val Loss: 0.0837 Acc: 0.9774\n",
      "New best model found!\n",
      "New record loss: 0.083652734375, previous record loss: 0.083683203125\n",
      "\n",
      "Epoch 10/13\n",
      "----------\n",
      "train Loss: 0.0829 Acc: 0.9780\n",
      "val Loss: 0.0839 Acc: 0.9774\n",
      "\n",
      "Epoch 11/13\n",
      "----------\n",
      "train Loss: 0.0834 Acc: 0.9781\n",
      "val Loss: 0.0844 Acc: 0.9776\n",
      "\n",
      "Epoch 12/13\n",
      "----------\n",
      "train Loss: 0.0832 Acc: 0.9782\n",
      "val Loss: 0.0843 Acc: 0.9772\n",
      "\n",
      "Epoch 13/13\n",
      "----------\n",
      "train Loss: 0.0820 Acc: 0.9784\n",
      "val Loss: 0.0843 Acc: 0.9770\n",
      "\n",
      "Training complete in 10m 25s\n",
      "Best val Acc: 0.9774 Best val loss: 0.0837\n"
     ]
    }
   ],
   "source": [
    "###################### Change as needed ######################\n",
    "num_epochs = 14\n",
    "##############################################################\n",
    "\n",
    "today = datetime.datetime.today() \n",
    "timestamp = today.strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Start the training\n",
    "model_conv, best_val_loss, best_val_acc = train_model(model_conv,\n",
    "                                                      criterion,\n",
    "                                                      optimizer_conv,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      num_epochs,\n",
    "                                                      timestamp)\n",
    "\n",
    "# Save the trained model for future use.\n",
    "torch.save({'model_state_dict': model_conv.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_conv.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\n",
    "            }, check_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End\n",
    "This is the end of Notebook 3. Please move forward to Notebook 4 where you will make prediction from our trained model in FP16.\n",
    "\n",
    "!!IMPORTANT!! To close this Notebook, you have to use File -> Close and Halt. With this way, the Python process associated with this Notebook will also be killed.\n",
    "\n",
    "Version 0.5  - January 9th, 2020 - ©2020 hanindhito@bagus.my.id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
