{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE382V - Hardware Architecture for Machine Learning\n",
    "## NVIDIA Tensor Cores for Accelerating Machine Learning Workload\n",
    "\n",
    "## Notebook 0 - Dataset Preparation\n",
    "\n",
    "In this notebook, we will download and prepare the dataset to train our neural network model. Our model should be able to distinguish between cats and dogs when it is given an image. To accomplish this task, we need a lot of dog and cat images labeled correctly so that we can feed them to train our model. Luckily, we do not need to build the dataset ourself. We can just download the dataset from internet.\n",
    "\n",
    "We will use dataset from Kaggle that contains 12,500 images of cats and 12,500 images of dogs [5]. You may think that 25,000 images are already a lot. In fact, more complex model may need million of images (data) to train so that it can accomplish complex tasks. This is where the big data comes useful for machine learning task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "We need to import some libraries which are needed to perform some functions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import split_folders\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variable\n",
    "Here, we define global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir       = './'\n",
    "raw_dir        = f'{data_dir}/raw'\n",
    "raw_dogs_dir   = f'{raw_dir}/dogs'\n",
    "raw_cats_dir   = f'{raw_dir}/cats'\n",
    "train_dir      = f'{data_dir}/train'\n",
    "train_dogs_dir = f'{train_dir}/dogs'\n",
    "train_cats_dir = f'{train_dir}/cats'\n",
    "val_dir        = f'{data_dir}/val'\n",
    "val_dogs_dir   = f'{val_dir}/dogs'\n",
    "val_cats_dir   = f'{val_dir}/cats'\n",
    "log_dir        = f'{data_dir}/log'\n",
    "chk_dir        = f'{data_dir}/checkpoint'\n",
    "test_dir       = f'{data_dir}/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset\n",
    "Let's download the training+validation dataset and test dataset. The training+validation dataset has a size of 543MB while the test dataset has a size of 271MB. It may take a while to download and extract the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Unzipping the Training+Validation Dataset\n",
    "gdd.download_file_from_google_drive(file_id='1TgS3BLPIoc3FHUBrvp6rXaz6g1UJz_2E',\n",
    "                                    dest_path='./raw.zip',\n",
    "                                    showsize=False,\n",
    "                                    overwrite=True,\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Unzipping the Testing Dataset\n",
    "gdd.download_file_from_google_drive(file_id='1JRMQY-gXp43ag65nP7HMNFEKhkJTxykw',\n",
    "                                    dest_path='./test.zip',\n",
    "                                    showsize=False,\n",
    "                                    overwrite=True,\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workspace Preparation\n",
    "We create new directory to process our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(raw_cats_dir   ,exist_ok=True)\n",
    "os.makedirs(raw_dogs_dir   ,exist_ok=True)\n",
    "os.makedirs(train_dir      ,exist_ok=True)\n",
    "os.makedirs(train_cats_dir ,exist_ok=True)\n",
    "os.makedirs(train_dogs_dir ,exist_ok=True)\n",
    "os.makedirs(val_dir        ,exist_ok=True)\n",
    "os.makedirs(val_cats_dir   ,exist_ok=True)\n",
    "os.makedirs(val_dogs_dir   ,exist_ok=True)\n",
    "os.makedirs(log_dir        ,exist_ok=True)\n",
    "os.makedirs(chk_dir        ,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Grouping\n",
    "Since we will work with two class of data: dogs and cats, it is a good practice to put all of the images of the same class in the same folder. Therefore, we will put all of dog images in dogs folder and all of cat images in cats folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(raw_dir)\n",
    "for f in files:\n",
    "    catImageList = re.search(\"cat\", f)\n",
    "    dogImageList = re.search(\"dog\", f)\n",
    "    if catImageList:\n",
    "        shutil.move(f'{raw_dir}/{f}', raw_cats_dir)\n",
    "    elif dogImageList:\n",
    "        shutil.move(f'{raw_dir}/{f}', raw_dogs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Splitting\n",
    "We need to split our dataset into training dataset and validation dataset. The training dataset is used to train our model and update our model parameter while the validation dataset is used to validate our model without updating our model parameter. In this way, we can see how our model performs when it encounters data that it has never seen before. Validation is also useful to see whether our model is overfit, that is it is only good for the data it has seen. Our training target is to get the best accuracy in our validation dataset.\n",
    "\n",
    "The training+validation dataset contains 25,000 images: 12,500 cat images and 12,500 dog images. We will split the dataset into training dataset and validation dataset. Usually, a good ratio is 80% for training, 20% for validation but you are free to change the number. You are also free to change the random seed to obtain new data splitting randomness. It will take a while to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset for training and validation\n",
    "\n",
    "###################### Change as needed ######################\n",
    "percentage_for_training   = 0.8\n",
    "percentage_for_validation = 0.2\n",
    "random_seed               = 12345\n",
    "##############################################################\n",
    "\n",
    "split_folders.ratio(f'{raw_dir}', output=\"./\", seed=random_seed, ratio=(percentage_for_training, percentage_for_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preview\n",
    "Let's check whether we have correct data in each dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the sample images in Cats Folder\n",
    "\n",
    "###################### Change as needed ######################\n",
    "num_of_cats_images = 5\n",
    "##############################################################\n",
    "\n",
    "cats_data_files = os.listdir(train_cats_dir)\n",
    "fig, ax = plt.subplots(num_of_cats_images, figsize=(num_of_cats_images*5, num_of_cats_images*5))\n",
    "fig.tight_layout(pad=5)\n",
    "image_displayed   = 0\n",
    "\n",
    "for fname in cats_data_files :    \n",
    "    im         = Image.open(f'{train_cats_dir}/{fname}')\n",
    "    ax[image_displayed].imshow(im)\n",
    "    ax[image_displayed].axis('on')\n",
    "    ax[image_displayed].set_title(fname)\n",
    "    image_displayed += 1\n",
    "    if(image_displayed>=num_of_cats_images) :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the sample images in Dogs Folder\n",
    "\n",
    "###################### Change as needed ######################\n",
    "num_of_dogs_images = 5\n",
    "##############################################################\n",
    "\n",
    "dogs_data_files = os.listdir(train_dogs_dir)\n",
    "fig, ax = plt.subplots(num_of_dogs_images, figsize=(num_of_dogs_images*5, num_of_dogs_images*5))\n",
    "fig.tight_layout(pad=5)\n",
    "image_displayed   = 0\n",
    "\n",
    "for fname in dogs_data_files :    \n",
    "    im         = Image.open(f'{train_dogs_dir}/{fname}')\n",
    "    ax[image_displayed].imshow(im)\n",
    "    ax[image_displayed].axis('on')\n",
    "    ax[image_displayed].set_title(fname)\n",
    "    image_displayed += 1\n",
    "    if(image_displayed>=num_of_dogs_images) :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End\n",
    "This is the end of Notebook 0. Please take a note that each images in our dataset has different size and we must resize them to fit into our neural network model. Please move forward to Notebook 1 where we will train our neural network model.\n",
    "\n",
    "\n",
    "Version 1.0  - January 5th, 2020 - Â©2020 hanindhito@bagus.my.id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
